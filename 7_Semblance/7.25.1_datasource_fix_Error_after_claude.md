@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DatasourceSetup (main) $ kubectl describe pod grafana-6fbbccf49f-hbnr4  -n grafana-monitoring
Name:             grafana-6fbbccf49f-hbnr4
Namespace:        grafana-monitoring
Priority:         0
Service Account:  grafana
Node:             minikube/192.168.49.2
Start Time:       Tue, 14 Jan 2025 15:53:31 +0000
Labels:           app.kubernetes.io/component=grafana
                  app.kubernetes.io/instance=grafana
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=grafana
                  app.kubernetes.io/version=11.4.0
                  helm.sh/chart=grafana-11.4.2
                  pod-template-hash=6fbbccf49f
Annotations:      checksum/config: 19c20dc5bd07bc522511ffab5d9eac1446dcd3a6450e7bbfdf09a5d562286cec
                  checksum/dashboard-provider: cd54948bf37e0721fd87481725f1618776aa7ba6f4259f9eb094412c9ca9d1f6
                  checksum/secret: 4392d22dc063d7ffda1126edc47da819eefc80a1e3130abeb719c2232887e531
Status:           Running
IP:               10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/grafana-6fbbccf49f
Containers:
  grafana:
    Container ID:    docker://4d974b1c58bb2fa5ecba0f9b03d29b28b14574ce7f0ab874ad4b3e59d0a435b5
    Image:           docker.io/bitnami/grafana:11.4.0-debian-12-r0
    Image ID:        docker-pullable://bitnami/grafana@sha256:cb8ab551567670a68f44052587c169339732fa7e0f4d806aecfd6b1e5889c626
    Port:            3000/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Waiting
      Reason:        CrashLoopBackOff
    Last State:      Terminated
      Reason:        Error
      Exit Code:     1
      Started:       Tue, 14 Jan 2025 15:57:34 +0000
      Finished:      Tue, 14 Jan 2025 15:57:40 +0000
    Ready:           False
    Restart Count:   5
    Limits:
      cpu:                150m
      ephemeral-storage:  2Gi
      memory:             192Mi
    Requests:
      cpu:                100m
      ephemeral-storage:  50Mi
      memory:             128Mi
    Liveness:             tcp-socket :dashboard delay=120s timeout=5s period=10s #success=1 #failure=6
    Readiness:            http-get http://:dashboard/api/health delay=30s timeout=5s period=10s #success=1 #failure=6
    Environment Variables from:
      grafana-envvars  ConfigMap  Optional: false
    Environment:
      GF_SECURITY_ADMIN_PASSWORD:  <set to the key 'GF_SECURITY_ADMIN_PASSWORD' in secret 'grafana-admin'>  Optional: false
    Mounts:
      /bitnami/grafana from empty-dir (rw,path="app-volume-dir")
      /opt/bitnami/grafana/conf from empty-dir (rw,path="app-conf-dir")
      /opt/bitnami/grafana/conf.default/provisioning/dashboards from dashboards-provider (rw)
      /opt/bitnami/grafana/conf.default/provisioning/datasources from datasources (rw)
      /opt/bitnami/grafana/dashboards/grafana_dashboard_config1.json from grafana-dashboard-config1 (rw,path="grafana_dashboard_config1.json")
      /opt/bitnami/grafana/dashboards/grafana_dashboard_config2.json from grafana-dashboard-config2 (rw,path="grafana_dashboard_config2.json")
      /opt/bitnami/grafana/data from data (rw)
      /opt/bitnami/grafana/tmp from empty-dir (rw,path="app-tmp-dir")
      /tmp from empty-dir (rw,path="tmp-dir")
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  empty-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  grafana
    ReadOnly:   false
  dashboards-provider:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-provider
    Optional:  false
  grafana-dashboard-config1:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-config1
    Optional:  false
  grafana-dashboard-config2:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-config2
    Optional:  false
  datasources:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  grafana-datasource-secret
    Optional:    false
  grafana-storage:
    Type:        EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:      
    SizeLimit:   <unset>
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                    From               Message
  ----     ------            ----                   ----               -------
  Warning  FailedScheduling  5m11s (x2 over 5m41s)  default-scheduler  0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         5m9s                   default-scheduler  Successfully assigned grafana-monitoring/grafana-6fbbccf49f-hbnr4 to minikube
  Normal   Pulling           5m9s                   kubelet            Pulling image "docker.io/bitnami/grafana:11.4.0-debian-12-r0"
  Normal   Pulled            4m46s                  kubelet            Successfully pulled image "docker.io/bitnami/grafana:11.4.0-debian-12-r0" in 22.057s (22.057s including waiting). Image size: 702398522 bytes.
  Normal   Created           2m38s (x5 over 4m44s)  kubelet            Created container grafana
  Normal   Pulled            2m38s (x4 over 4m29s)  kubelet            Container image "docker.io/bitnami/grafana:11.4.0-debian-12-r0" already present on machine
  Normal   Started           2m37s (x5 over 4m44s)  kubelet            Started container grafana
  Warning  BackOff           5s (x23 over 4m23s)    kubelet            Back-off restarting failed container grafana in pod grafana-6fbbccf49f-hbnr4_grafana-monitoring(6d4bb734-b5ec-454f-a2c3-40625caebd32)
@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DatasourceSetup (main) $ 

Resolution>
Let me help analyze and fix the issues with your Grafana pod that's in CrashLoopBackOff state. There are several key issues to address:

1. First, let's check the pod logs to understand why it's crashing:

```bash
kubectl logs grafana-6fbbccf49f-hbnr4 -n grafana-monitoring
```

2. From the pod description, I can see a few potential issues:

a. PersistentVolume Issue:
```yaml
Warning  FailedScheduling  5m11s (x2 over 5m41s)  default-scheduler  0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims
```

Let's create a PersistentVolume and PersistentVolumeClaim:



```yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: grafana-pv
  namespace: grafana-monitoring
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/grafana-storage
  persistentVolumeReclaimPolicy: Retain
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana
  namespace: grafana-monitoring
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

```

3. Apply the storage configuration:
```bash
kubectl apply -f grafana-storage.yaml
```

4. The pod also shows resource constraints that might be too tight. Let's update the deployment with adjusted resources:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: grafana-monitoring
spec:
  template:
    spec:
      containers:
      - name: grafana
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 250m
            memory: 256Mi

```

5. Apply the resource updates:
```bash
kubectl apply -f grafana-deployment-patch.yaml
```

6. Verify permissions on mounted volumes:
```bash
kubectl exec -it grafana-6fbbccf49f-hbnr4 -n grafana-monitoring -- ls -la /bitnami/grafana
```

Additional steps to check:

1. Verify your datasource secret is correctly formatted:
```bash
kubectl get secret grafana-datasource-secret -n grafana-monitoring -o yaml
```

2. Check if ConfigMaps are properly created:
```bash
kubectl get configmap -n grafana-monitoring
```

3. Delete the pod to force a fresh deployment:
```bash
kubectl delete pod grafana-6fbbccf49f-hbnr4 -n grafana-monitoring
```

If the issue persists after applying these fixes, please share the pod logs so I can provide more specific guidance. The logs will show the exact error causing the crashes.

Would you like me to explain any of these steps in more detail or help with interpreting the logs?

---

Claude output

@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DatasourceSetup (main) $ kubectl logs grafana-6fbbccf49f-hbnr4 -n grafana-monitoring
grafana 16:00:31.12 INFO  ==> 
grafana 16:00:31.12 INFO  ==> Welcome to the Bitnami grafana container
grafana 16:00:31.12 INFO  ==> Subscribe to project updates by watching https://github.com/bitnami/containers
grafana 16:00:31.12 INFO  ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
grafana 16:00:31.12 INFO  ==> 
grafana 16:00:31.22 INFO  ==> ** Grafana setup finished! **

grafana 16:00:31.23 INFO  ==> ** Starting Grafana **
Error checking server process execution privilege. error: could not get current OS user to detect process privileges
logger=settings t=2025-01-14T16:00:32.029749828Z level=info msg="Starting Grafana" version=11.3.1 commit=b5870186 branch=HEAD compiled=2024-12-04T21:19:30Z
logger=settings t=2025-01-14T16:00:32.030106833Z level=info msg="Config loaded from" file=/opt/bitnami/grafana/conf/defaults.ini
logger=settings t=2025-01-14T16:00:32.030121741Z level=info msg="Config loaded from" file=/opt/bitnami/grafana/conf/grafana.ini
logger=settings t=2025-01-14T16:00:32.030127853Z level=info msg="Config overridden from command line" arg="default.paths.data=/opt/bitnami/grafana/data"
logger=settings t=2025-01-14T16:00:32.030133252Z level=info msg="Config overridden from command line" arg="default.paths.logs=/opt/bitnami/grafana/logs"
logger=settings t=2025-01-14T16:00:32.030139785Z level=info msg="Config overridden from command line" arg="default.paths.plugins=/opt/bitnami/grafana/data/plugins"
logger=settings t=2025-01-14T16:00:32.030145165Z level=info msg="Config overridden from command line" arg="default.paths.provisioning=/opt/bitnami/grafana/conf/provisioning"
logger=settings t=2025-01-14T16:00:32.030151276Z level=info msg="Config overridden from command line" arg="default.log.mode=console"
logger=settings t=2025-01-14T16:00:32.030156807Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_DATA=/opt/bitnami/grafana/data"
logger=settings t=2025-01-14T16:00:32.030162226Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_LOGS=/opt/bitnami/grafana/logs"
logger=settings t=2025-01-14T16:00:32.030167006Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PLUGINS=/opt/bitnami/grafana/data/plugins"
logger=settings t=2025-01-14T16:00:32.030171955Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PROVISIONING=/opt/bitnami/grafana/conf/provisioning"
logger=settings t=2025-01-14T16:00:32.030176954Z level=info msg="Config overridden from Environment variable" var="GF_SECURITY_ADMIN_USER=admin"
logger=settings t=2025-01-14T16:00:32.030181703Z level=info msg="Config overridden from Environment variable" var="GF_SECURITY_ADMIN_PASSWORD=*********"
logger=settings t=2025-01-14T16:00:32.030187293Z level=info msg="Config overridden from Environment variable" var="GF_AUTH_LDAP_ENABLED=false"
logger=settings t=2025-01-14T16:00:32.030192683Z level=info msg="Config overridden from Environment variable" var="GF_AUTH_LDAP_CONFIG_FILE=/opt/bitnami/grafana/conf/ldap.toml"
logger=settings t=2025-01-14T16:00:32.030197833Z level=info msg="Config overridden from Environment variable" var="GF_AUTH_LDAP_ALLOW_SIGN_UP=false"
logger=settings t=2025-01-14T16:00:32.030202912Z level=info msg=Target target=[all]
logger=settings t=2025-01-14T16:00:32.030223912Z level=info msg="Path Home" path=/opt/bitnami/grafana
logger=settings t=2025-01-14T16:00:32.030229161Z level=info msg="Path Data" path=/opt/bitnami/grafana/data
logger=settings t=2025-01-14T16:00:32.03023404Z level=info msg="Path Logs" path=/opt/bitnami/grafana/logs
logger=settings t=2025-01-14T16:00:32.030238759Z level=info msg="Path Plugins" path=/opt/bitnami/grafana/data/plugins
logger=settings t=2025-01-14T16:00:32.030255981Z level=info msg="Path Provisioning" path=/opt/bitnami/grafana/conf/provisioning
logger=settings t=2025-01-14T16:00:32.030261592Z level=info msg="App mode production"
logger=featuremgmt t=2025-01-14T16:00:32.030644796Z level=info msg=FeatureToggles transformationsVariableSupport=true nestedFolders=true kubernetesPlaylists=true addFieldFromCalculationStatFunctions=true exploreMetrics=true lokiQuerySplitting=true openSearchBackendFlowEnabled=true logRowsPopoverMenu=true recordedQueriesMulti=true formatString=true dashboardSceneSolo=true transformationsRedesign=true lokiMetricDataplane=true prometheusAzureOverrideAudience=true notificationBanner=true dataplaneFrontendFallback=true dashgpt=true accessControlOnCall=true alertingSimplifiedRouting=true angularDeprecationUI=true lokiQueryHints=true alertingNoDataErrorExecution=true cloudWatchRoundUpEndTime=true correlations=true prometheusConfigOverhaulAuth=true tlsMemcached=true dashboardScene=true cloudWatchCrossAccountQuerying=true logsExploreTableVisualisation=true prometheusMetricEncyclopedia=true panelMonitoring=true annotationPermissionUpdate=true groupToNestedTableTransformation=true awsAsyncQueryCaching=true logsInfiniteScrolling=true dashboardSceneForViewers=true autoMigrateXYChartPanel=true influxdbBackendMigration=true cloudWatchNewLabelParsing=true logsContextDatasourceUi=true managedPluginsInstall=true topnav=true ssoSettingsApi=true pinNavItems=true promQLScope=true recoveryThreshold=true publicDashboardsScene=true publicDashboards=true alertingInsights=true lokiStructuredMetadata=true
logger=sqlstore t=2025-01-14T16:00:32.030708725Z level=info msg="Connecting to DB" dbtype=sqlite3
logger=migrator t=2025-01-14T16:00:32.033302769Z level=info msg="Locking database"
logger=migrator t=2025-01-14T16:00:32.033317487Z level=info msg="Starting DB migrations"
logger=migrator t=2025-01-14T16:00:32.122162382Z level=info msg="migrations completed" performed=0 skipped=611 duration=759.766µs
logger=migrator t=2025-01-14T16:00:32.122486957Z level=info msg="Unlocking database"
logger=secrets t=2025-01-14T16:00:32.123209303Z level=info msg="Envelope encryption state" enabled=true currentprovider=secretKey.v1
logger=plugin.angulardetectorsprovider.dynamic t=2025-01-14T16:00:32.637369269Z level=info msg="Restored cache from database" duration=403.1µs
logger=plugin.store t=2025-01-14T16:00:32.638407123Z level=info msg="Loading plugins..."
logger=plugins.registration t=2025-01-14T16:00:33.19682227Z level=error msg="Could not register plugin" pluginId=xychart error="plugin xychart is already registered"
logger=plugins.initialization t=2025-01-14T16:00:33.196863817Z level=error msg="Could not initialize plugin" pluginId=xychart error="plugin xychart is already registered"
logger=local.finder t=2025-01-14T16:00:33.196938897Z level=warn msg="Skipping finding plugins as directory does not exist" path=/opt/bitnami/grafana/plugins-bundled
logger=plugins.validator.angular t=2025-01-14T16:00:34.021641885Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=agenty-flowcharting-panel
logger=plugins.validation t=2025-01-14T16:00:34.02166578Z level=error msg="Plugin validation failed" pluginId=agenty-flowcharting-panel error="angular plugins are not supported"
logger=plugins.validator.angular t=2025-01-14T16:00:34.132876234Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=briangann-datatable-panel
logger=plugins.validation t=2025-01-14T16:00:34.132899778Z level=error msg="Plugin validation failed" pluginId=briangann-datatable-panel error="angular plugins are not supported"
logger=plugins.registration t=2025-01-14T16:00:34.295320874Z level=info msg="Plugin registered" pluginId=briangann-gauge-panel
logger=plugins.validator.angular t=2025-01-14T16:00:34.323469892Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=digiapulssi-organisations-panel
logger=plugins.validation t=2025-01-14T16:00:34.323491803Z level=error msg="Plugin validation failed" pluginId=digiapulssi-organisations-panel error="angular plugins are not supported"
logger=plugins.registration t=2025-01-14T16:00:34.600538459Z level=info msg="Plugin registered" pluginId=grafana-clock-panel
logger=plugins.validator.angular t=2025-01-14T16:00:34.70174463Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=grafana-piechart-panel
logger=plugins.validation t=2025-01-14T16:00:34.701765288Z level=error msg="Plugin validation failed" pluginId=grafana-piechart-panel error="angular plugins are not supported"
logger=plugins.registration t=2025-01-14T16:00:35.001882939Z level=info msg="Plugin registered" pluginId=grafana-polystat-panel
logger=plugins.registration t=2025-01-14T16:00:35.49856237Z level=info msg="Plugin registered" pluginId=jdbranham-diagram-panel
logger=plugins.registration t=2025-01-14T16:00:35.597056177Z level=info msg="Plugin registered" pluginId=larona-epict-panel
logger=plugins.validator.angular t=2025-01-14T16:00:35.73622112Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=marcuscalidus-svg-panel
logger=plugins.validation t=2025-01-14T16:00:35.736243672Z level=error msg="Plugin validation failed" pluginId=marcuscalidus-svg-panel error="angular plugins are not supported"
logger=plugins.validator.angular t=2025-01-14T16:00:35.830448876Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=michaeldmoore-annunciator-panel
logger=plugins.validation t=2025-01-14T16:00:35.830485154Z level=error msg="Plugin validation failed" pluginId=michaeldmoore-annunciator-panel error="angular plugins are not supported"
logger=plugins.validator.angular t=2025-01-14T16:00:35.928717043Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=michaeldmoore-multistat-panel
logger=plugins.validation t=2025-01-14T16:00:35.928745265Z level=error msg="Plugin validation failed" pluginId=michaeldmoore-multistat-panel error="angular plugins are not supported"
logger=plugins.validator.angular t=2025-01-14T16:00:36.020466322Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=natel-discrete-panel
logger=plugins.validation t=2025-01-14T16:00:36.020488534Z level=error msg="Plugin validation failed" pluginId=natel-discrete-panel error="angular plugins are not supported"
logger=plugins.validator.angular t=2025-01-14T16:00:36.120128365Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=neocat-cal-heatmap-panel
logger=plugins.validation t=2025-01-14T16:00:36.120149975Z level=error msg="Plugin validation failed" pluginId=neocat-cal-heatmap-panel error="angular plugins are not supported"
logger=plugins.registration t=2025-01-14T16:00:36.420121636Z level=info msg="Plugin registered" pluginId=pierosavi-imageit-panel
logger=plugins.registration t=2025-01-14T16:00:36.571112277Z level=info msg="Plugin registered" pluginId=scadavis-synoptic-panel
logger=plugins.validator.angular t=2025-01-14T16:00:36.700613709Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=snuids-trafficlights-panel
logger=plugins.validation t=2025-01-14T16:00:36.700639848Z level=error msg="Plugin validation failed" pluginId=snuids-trafficlights-panel error="angular plugins are not supported"
logger=plugins.registration t=2025-01-14T16:00:36.722352062Z level=info msg="Plugin registered" pluginId=vonage-status-panel
logger=plugin.store t=2025-01-14T16:00:36.722374314Z level=info msg="Plugins loaded" count=62 duration=4.083968452s
logger=query_data t=2025-01-14T16:00:36.901390134Z level=info msg="Query Service initialization"
logger=live.push_http t=2025-01-14T16:00:36.921457673Z level=info msg="Live Push Gateway initialization"
logger=ngalert.notifier.alertmanager org=1 t=2025-01-14T16:00:36.994533462Z level=info msg="Applying new configuration to Alertmanager" configHash=d2c56faca6af2a5772ff4253222f7386
logger=ngalert.state.manager t=2025-01-14T16:00:37.000820206Z level=info msg="Running in alternative execution of Error/NoData mode"
logger=infra.usagestats.collector t=2025-01-14T16:00:37.002709539Z level=info msg="registering usage stat providers" usageStatsProvidersLen=2
logger=server t=2025-01-14T16:00:37.002933306Z level=info msg="Writing PID file" path=/opt/bitnami/grafana/tmp/grafana.pid pid=1
logger=provisioning t=2025-01-14T16:00:37.003500393Z level=error msg="Failed to provision data sources" error="Datasource provisioning error: yaml: unmarshal errors:\n  line 1: cannot unmarshal !!str `v1` into int64"
logger=provisioning t=2025-01-14T16:00:37.003515922Z level=error msg="Failed to provision data sources" error="Datasource provisioning error: yaml: unmarshal errors:\n  line 1: cannot unmarshal !!str `v1` into int64"
Error: ✗ Datasource provisioning error: yaml: unmarshal errors:
  line 1: cannot unmarshal !!str `v1` into int64
@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DatasourceSetup (main

---

I see the issue in the logs. The main error is with the datasource provisioning:

```
Error: ✗ Datasource provisioning error: yaml: unmarshal errors:
  line 1: cannot unmarshal !!str `v1` into int64
```

This is happening because the `apiVersion` in your datasource configuration is incorrectly formatted. Let me provide the correct datasource secret configuration:



```yaml
apiVersion: v1
kind: Secret
metadata:
  name: grafana-datasource-secret
  namespace: grafana-monitoring
type: Opaque
stringData:
  datasource.yaml: |-
    apiVersion: 1
    deleteDatasources:
      - name: Prometheus
        orgId: 1
    datasources:
      - name: Prometheus
        type: prometheus
        access: proxy
        url: http://prometheus:9090
        isDefault: true
        version: 1
        editable: true
        jsonData:
          timeInterval: "5s"
          httpMethod: "POST"
        secureJsonData:
          basicAuthPassword: "password"
          basicAuthUser: "user"
          tlsSkipVerify: true
      - name: Random Walk
        type: grafana-random-walk
        access: proxy
        url: http://random-walk:8080
        isDefault: false
        version: 1
        editable: true
        jsonData:
          timeInterval: "5s"
        secureJsonData:
          basicAuthPassword: "password"
          basicAuthUser: "user"
          tlsSkipVerify: true

```

Key changes made:
1. Used `stringData` instead of `data` (no base64 encoding needed)
2. Fixed the `apiVersion` format in the datasource.yaml
3. Added proper structure for datasource configuration
4. Added `version` and `editable` fields
5. Properly formatted the JSON data sections

Apply the new configuration:

```bash
kubectl apply -f grafana-datasource-secret.yaml
```

Then delete the pod to force a restart:

```bash
kubectl delete pod grafana-6fbbccf49f-hbnr4 -n grafana-monitoring
```

The new pod should come up without the datasource provisioning error. If you continue to see issues, please share the new logs after applying these changes.

Note: Make sure your Prometheus service is actually running at `http://prometheus:9090` in your cluster. If it's in a different namespace or has a different service name, you'll need to adjust the URL accordingly.

Would you like me to explain any part of the configuration in more detail?


second try


rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DatasourceSetup (main) $ kubectl describe pod grafana-6fbbccf49f-zw9gl  -n grafana-monitoring
Name:             grafana-6fbbccf49f-zw9gl
Namespace:        grafana-monitoring
Priority:         0
Service Account:  grafana
Node:             minikube/192.168.49.2
Start Time:       Tue, 14 Jan 2025 16:07:56 +0000
Labels:           app.kubernetes.io/component=grafana
                  app.kubernetes.io/instance=grafana
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=grafana
                  app.kubernetes.io/version=11.4.0
                  helm.sh/chart=grafana-11.4.2
                  pod-template-hash=6fbbccf49f
Annotations:      checksum/config: 19c20dc5bd07bc522511ffab5d9eac1446dcd3a6450e7bbfdf09a5d562286cec
                  checksum/dashboard-provider: cd54948bf37e0721fd87481725f1618776aa7ba6f4259f9eb094412c9ca9d1f6
                  checksum/secret: 4392d22dc063d7ffda1126edc47da819eefc80a1e3130abeb719c2232887e531
Status:           Running
IP:               10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/grafana-6fbbccf49f
Containers:
  grafana:
    Container ID:    docker://d2bc382d3b5d760fb8e98d4d90c421fe183236779835261684bb136dfcb5bbe3
    Image:           docker.io/bitnami/grafana:11.4.0-debian-12-r0
    Image ID:        docker-pullable://bitnami/grafana@sha256:cb8ab551567670a68f44052587c169339732fa7e0f4d806aecfd6b1e5889c626
    Port:            3000/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Waiting
      Reason:        CrashLoopBackOff
    Last State:      Terminated
      Reason:        Error
      Exit Code:     1
      Started:       Tue, 14 Jan 2025 16:08:58 +0000
      Finished:      Tue, 14 Jan 2025 16:09:04 +0000
    Ready:           False
    Restart Count:   2
    Limits:
      cpu:                150m
      ephemeral-storage:  2Gi
      memory:             192Mi
    Requests:
      cpu:                100m
      ephemeral-storage:  50Mi
      memory:             128Mi
    Liveness:             tcp-socket :dashboard delay=120s timeout=5s period=10s #success=1 #failure=6
    Readiness:            http-get http://:dashboard/api/health delay=30s timeout=5s period=10s #success=1 #failure=6
    Environment Variables from:
      grafana-envvars  ConfigMap  Optional: false
    Environment:
      GF_SECURITY_ADMIN_PASSWORD:  <set to the key 'GF_SECURITY_ADMIN_PASSWORD' in secret 'grafana-admin'>  Optional: false
    Mounts:
      /bitnami/grafana from empty-dir (rw,path="app-volume-dir")
      /opt/bitnami/grafana/conf from empty-dir (rw,path="app-conf-dir")
      /opt/bitnami/grafana/conf.default/provisioning/dashboards from dashboards-provider (rw)
      /opt/bitnami/grafana/conf.default/provisioning/datasources from datasources (rw)
      /opt/bitnami/grafana/dashboards/grafana_dashboard_config1.json from grafana-dashboard-config1 (rw,path="grafana_dashboard_config1.json")
      /opt/bitnami/grafana/dashboards/grafana_dashboard_config2.json from grafana-dashboard-config2 (rw,path="grafana_dashboard_config2.json")
      /opt/bitnami/grafana/data from data (rw)
      /opt/bitnami/grafana/tmp from empty-dir (rw,path="app-tmp-dir")
      /tmp from empty-dir (rw,path="tmp-dir")
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  empty-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  grafana
    ReadOnly:   false
  dashboards-provider:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-provider
    Optional:  false
  grafana-dashboard-config1:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-config1
    Optional:  false
  grafana-dashboard-config2:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-config2
    Optional:  false
  datasources:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  grafana-datasource-secret
    Optional:    false
  grafana-storage:
    Type:        EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:      
    SizeLimit:   <unset>
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  95s (x2 over 2m4s)  default-scheduler  0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         92s                 default-scheduler  Successfully assigned grafana-monitoring/grafana-6fbbccf49f-zw9gl to minikube
  Normal   Pulling           92s                 kubelet            Pulling image "docker.io/bitnami/grafana:11.4.0-debian-12-r0"
  Normal   Pulled            68s                 kubelet            Successfully pulled image "docker.io/bitnami/grafana:11.4.0-debian-12-r0" in 23.335s (23.335s including waiting). Image size: 702398522 bytes.
  Normal   Created           30s (x3 over 68s)   kubelet            Created container grafana
  Normal   Pulled            30s (x2 over 51s)   kubelet            Container image "docker.io/bitnami/grafana:11.4.0-debian-12-r0" already present on machine
  Normal   Started           29s (x3 over 68s)   kubelet            Started container grafana
  Warning  BackOff           10s (x5 over 44s)   kubelet            Back-off restarting failed container grafana in pod grafana-6fbbccf49f-zw9gl_grafana-monitoring(a47b41e5-8827-4725-ab31-1a90fa50f057)

  ---


  I see the main issue from the events - the pod has unbound PersistentVolumeClaims. Let's fix this step by step.



```yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: grafana-pv
  namespace: grafana-monitoring
  labels:
    type: local
spec:
  storageClassName: standard
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data/grafana"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana
  namespace: grafana-monitoring
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

```

Follow these steps to fix the storage issue:

1. First, create the directory on your Minikube node:
```bash
minikube ssh
sudo mkdir -p /mnt/data/grafana
sudo chmod 777 /mnt/data/grafana
exit
```

2. Apply the storage configuration:
```bash
kubectl apply -f grafana-storage-config.yaml
```

3. Verify the PV and PVC are bound:
```bash
kubectl get pv,pvc -n grafana-monitoring
```

4. Delete the existing pod to force recreation:
```bash
kubectl delete pod grafana-6fbbccf49f-zw9gl -n grafana-monitoring
```

5. Check if the new pod comes up properly:
```bash
kubectl get pods -n grafana-monitoring -w
```

Also, let's check your other resources to ensure they're properly configured:

```bash
# Check the ConfigMaps
kubectl get configmaps -n grafana-monitoring

# Check the Secrets
kubectl get secrets -n grafana-monitoring

# Check the Deployment
kubectl get deployment grafana -n grafana-monitoring
```

If the pod still crashes after fixing the storage, please share the new pod logs:
```bash
kubectl logs $(kubectl get pods -n grafana-monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].metadata.name}') -n grafana-monitoring
```

This should resolve the PersistentVolume issue, but if you continue to see other errors, we can address those next. Would you like me to explain any part of these configurations in more detail?

---

Let me help you update the deployment configuration to match the Bitnami Grafana parameters you've provided.



```yaml
## Grafana Configuration
##
grafana:
  ## Environment variables
  ##
  envVars:
    - name: GF_LOG_LEVEL
      value: debug

  ## Number of Grafana replicas
  ##
  replicaCount: 1

  ## Persistence configuration
  ##
  persistence:
    enabled: true
    size: 100Mi
    ## If defined, storageClassName: <storageClass>
    storageClassName: "standard"
    accessModes:
      - ReadWriteOnce

  ## Service configuration
  ##
  service:
    type: NodePort
    port: 3000
    nodePort: 30000

  ## Additional volumes
  ##
  extraVolumes:
    - name: grafana-storage
      emptyDir: {}
    - name: datasources
      secret:
        secretName: grafana-datasource-secret

  ## Volume mounts
  ##
  volumeMounts:
    - name: grafana-storage
      mountPath: /opt/bitnami/grafana
    - name: datasources
      mountPath: /opt/bitnami/grafana/conf/provisioning/datasources
      readOnly: true

  ## Resources configuration
  ##
  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi

  ## Configure additional datasources
  ##
  datasources:
    secretName: grafana-datasource-secret

```

Now, let's also create the corresponding datasource secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: grafana-datasource-secret
  namespace: grafana-monitoring
type: Opaque
stringData:
  datasource.yaml: |-
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        access: proxy
        url: http://prometheus:9090
        isDefault: true
        jsonData:
          timeInterval: "5s"
          httpMethod: "POST"
        secureJsonData:
          tlsSkipVerify: true

```

To apply these configurations:

1. Save the first configuration as `values.yaml`:
```bash
helm upgrade --install grafana bitnami/grafana -f values.yaml -n grafana-monitoring
```

2. Apply the datasource secret:
```bash
kubectl apply -f datasource-secret.yaml -n grafana-monitoring
```

Key changes made:
1. Structured the configuration according to Bitnami Grafana chart requirements
2. Added proper volume mounts for datasources
3. Configured persistence with the specified 100Mi size
4. Set up NodePort service with port 30000
5. Added debug logging
6. Configured reasonable resource limits
7. Set up proper datasource mounting

You can verify the deployment with:
```bash
kubectl get pods,svc -n grafana-monitoring
```

And check the logs with:
```bash
kubectl logs -f deployment/grafana -n grafana-monitoring
```

Would you like me to explain any part of these configurations in more detail?

---
rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DatasourceSetup (main) $ kubectl logs grafana-9949b85c5-t2fd2  -n grafana-monitoring
grafana 16:21:21.88 INFO  ==> 
grafana 16:21:21.89 INFO  ==> Welcome to the Bitnami grafana container
grafana 16:21:21.90 INFO  ==> Subscribe to project updates by watching https://github.com/bitnami/containers
grafana 16:21:21.90 INFO  ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
grafana 16:21:21.90 INFO  ==> 
grafana 16:21:21.99 INFO  ==> ** Grafana setup finished! **

grafana 16:21:22.00 INFO  ==> ** Starting Grafana **
Error checking server process execution privilege. error: could not get current OS user to detect process privileges
logger=settings t=2025-01-14T16:21:22.671718999Z level=info msg="Starting Grafana" version=11.3.1 commit=b5870186 branch=HEAD compiled=2024-12-04T21:19:30Z
logger=settings t=2025-01-14T16:21:22.672048092Z level=info msg="Config loaded from" file=/opt/bitnami/grafana/conf/defaults.ini
logger=settings t=2025-01-14T16:21:22.672063962Z level=info msg="Config loaded from" file=/opt/bitnami/grafana/conf/grafana.ini
logger=settings t=2025-01-14T16:21:22.672070534Z level=info msg="Config overridden from command line" arg="default.paths.data=/opt/bitnami/grafana/data"
logger=settings t=2025-01-14T16:21:22.672076515Z level=info msg="Config overridden from command line" arg="default.paths.logs=/opt/bitnami/grafana/logs"
logger=settings t=2025-01-14T16:21:22.672082276Z level=info msg="Config overridden from command line" arg="default.paths.plugins=/opt/bitnami/grafana/data/plugins"
logger=settings t=2025-01-14T16:21:22.672087416Z level=info msg="Config overridden from command line" arg="default.paths.provisioning=/opt/bitnami/grafana/conf/provisioning"
logger=settings t=2025-01-14T16:21:22.672092946Z level=info msg="Config overridden from command line" arg="default.log.mode=console"
logger=settings t=2025-01-14T16:21:22.672100781Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_DATA=/opt/bitnami/grafana/data"
logger=settings t=2025-01-14T16:21:22.67210584Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_LOGS=/opt/bitnami/grafana/logs"
logger=settings t=2025-01-14T16:21:22.672111961Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PLUGINS=/opt/bitnami/grafana/data/plugins"
logger=settings t=2025-01-14T16:21:22.672117191Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PROVISIONING=/opt/bitnami/grafana/conf/provisioning"
logger=settings t=2025-01-14T16:21:22.672122501Z level=info msg="Config overridden from Environment variable" var="GF_SECURITY_ADMIN_USER=admin"
logger=settings t=2025-01-14T16:21:22.67212768Z level=info msg="Config overridden from Environment variable" var="GF_SECURITY_ADMIN_PASSWORD=*********"
logger=settings t=2025-01-14T16:21:22.672133301Z level=info msg="Config overridden from Environment variable" var="GF_AUTH_LDAP_ENABLED=false"
logger=settings t=2025-01-14T16:21:22.672139513Z level=info msg="Config overridden from Environment variable" var="GF_AUTH_LDAP_CONFIG_FILE=/opt/bitnami/grafana/conf/ldap.toml"
logger=settings t=2025-01-14T16:21:22.672146125Z level=info msg="Config overridden from Environment variable" var="GF_AUTH_LDAP_ALLOW_SIGN_UP=false"
logger=settings t=2025-01-14T16:21:22.672151615Z level=info msg=Target target=[all]
logger=settings t=2025-01-14T16:21:22.672163437Z level=info msg="Path Home" path=/opt/bitnami/grafana
logger=settings t=2025-01-14T16:21:22.672169178Z level=info msg="Path Data" path=/opt/bitnami/grafana/data
logger=settings t=2025-01-14T16:21:22.672175139Z level=info msg="Path Logs" path=/opt/bitnami/grafana/logs
logger=settings t=2025-01-14T16:21:22.672179978Z level=info msg="Path Plugins" path=/opt/bitnami/grafana/data/plugins
logger=settings t=2025-01-14T16:21:22.672184727Z level=info msg="Path Provisioning" path=/opt/bitnami/grafana/conf/provisioning
logger=settings t=2025-01-14T16:21:22.672189987Z level=info msg="App mode production"
logger=featuremgmt t=2025-01-14T16:21:22.672552402Z level=info msg=FeatureToggles dashgpt=true exploreMetrics=true pinNavItems=true tlsMemcached=true addFieldFromCalculationStatFunctions=true cloudWatchRoundUpEndTime=true transformationsVariableSupport=true publicDashboards=true awsAsyncQueryCaching=true logsExploreTableVisualisation=true lokiQuerySplitting=true annotationPermissionUpdate=true prometheusConfigOverhaulAuth=true alertingInsights=true kubernetesPlaylists=true cloudWatchNewLabelParsing=true managedPluginsInstall=true ssoSettingsApi=true lokiQueryHints=true openSearchBackendFlowEnabled=true autoMigrateXYChartPanel=true lokiMetricDataplane=true transformationsRedesign=true prometheusMetricEncyclopedia=true logRowsPopoverMenu=true nestedFolders=true dataplaneFrontendFallback=true panelMonitoring=true angularDeprecationUI=true recordedQueriesMulti=true accessControlOnCall=true alertingSimplifiedRouting=true notificationBanner=true dashboardSceneForViewers=true prometheusAzureOverrideAudience=true logsInfiniteScrolling=true groupToNestedTableTransformation=true lokiStructuredMetadata=true alertingNoDataErrorExecution=true correlations=true dashboardScene=true formatString=true cloudWatchCrossAccountQuerying=true topnav=true dashboardSceneSolo=true publicDashboardsScene=true influxdbBackendMigration=true recoveryThreshold=true promQLScope=true logsContextDatasourceUi=true
logger=sqlstore t=2025-01-14T16:21:22.672616522Z level=info msg="Connecting to DB" dbtype=sqlite3
logger=migrator t=2025-01-14T16:21:22.675133713Z level=info msg="Locking database"
logger=migrator t=2025-01-14T16:21:22.675157738Z level=info msg="Starting DB migrations"
logger=migrator t=2025-01-14T16:21:22.699970742Z level=info msg="migrations completed" performed=0 skipped=611 duration=486.577µs
logger=migrator t=2025-01-14T16:21:22.700169933Z level=info msg="Unlocking database"
logger=secrets t=2025-01-14T16:21:22.767006352Z level=info msg="Envelope encryption state" enabled=true currentprovider=secretKey.v1
logger=plugin.angulardetectorsprovider.dynamic t=2025-01-14T16:21:23.173208697Z level=info msg="Restored cache from database" duration=308.755µs
logger=plugin.store t=2025-01-14T16:21:23.174136426Z level=info msg="Loading plugins..."
logger=plugins.registration t=2025-01-14T16:21:23.402411969Z level=error msg="Could not register plugin" pluginId=xychart error="plugin xychart is already registered"
logger=plugins.initialization t=2025-01-14T16:21:23.402444579Z level=error msg="Could not initialize plugin" pluginId=xychart error="plugin xychart is already registered"
logger=local.finder t=2025-01-14T16:21:23.40252022Z level=warn msg="Skipping finding plugins as directory does not exist" path=/opt/bitnami/grafana/plugins-bundled
logger=plugins.validator.angular t=2025-01-14T16:21:23.896943926Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=agenty-flowcharting-panel
logger=plugins.validation t=2025-01-14T16:21:23.896968602Z level=error msg="Plugin validation failed" pluginId=agenty-flowcharting-panel error="angular plugins are not supported"
logger=plugins.validator.angular t=2025-01-14T16:21:23.995829322Z level=error msg="Refusing to initialize plugin because it's using Angular, which has been disabled" pluginId=briangann-datatable-panel
logger=plugins.validation t=2025-01-14T16:21:23.9958604Z level=error msg="Plugin validation failed" pluginId=briangann-datatable-panel error="angular plugins are not supported"
@rifaterdemsahin ➜ /workspaces/grafana-dashboard/6_Symbols/4_DatasourceSetup (main) $ 

It looks like you're working with a Grafana setup in a Kubernetes environment. From the logs, Grafana has started successfully, but there are a few issues:

1. **Error checking server process execution privilege**: This error indicates that Grafana couldn't get the current OS user to detect process privileges. This might be related to permissions or user configurations in your Kubernetes setup.

2. **Plugin registration errors**: The logs show that the `xychart` plugin is already registered, causing an error. Additionally, some plugins using Angular (`agenty-flowcharting-panel` and `briangann-datatable-panel`) are not supported and failed to initialize.

To address these issues, you might want to:
- Check the permissions and user configurations in your Kubernetes setup to resolve the process privilege error.
- Remove or update the conflicting plugins to avoid registration errors.
- Replace or update plugins that rely on Angular, as they are not supported in your current Grafana setup.

If you need more detailed assistance, feel free to ask!