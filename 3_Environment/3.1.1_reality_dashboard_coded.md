# Environment( Reality ) : Grafana Coded Dashboard

## push commands
```bash
git pull; git add . ; git commit -m "Codespaces Host" -m "Environment updates"; git push;sleep 3;clear
```

## Section 1 : Start Setups
### Commands
- minikube delete # ðŸ˜Š Optional code block > Cant create quorom so delete it 
- minikube start --memory=4096 --cpus=2 # ðŸ“Œ mandatory

### Configuration
- minikube --memory=4096 --cpus=2
- minikube --cpus=2

### Outputs
```sh
@rifaterdemsahin âžœ /workspaces/grafana-dashboard (main) $ minikube delete
ðŸ”¥  Deleting "minikube" in docker ...
ðŸ”¥  Deleting container "minikube" ...
ðŸ”¥  Removing /home/codespace/.minikube/machines/minikube ...
ðŸ’€  Removed all traces of the "minikube" cluster.
```
```sh
@rifaterdemsahin âžœ /workspaces/grafana-dashboard (main) $ minikube start --memory=4096 --cpus=2
ðŸ˜„  minikube v1.34.0 on Ubuntu 20.04 (docker/amd64)
âœ¨  Automatically selected the docker driver. Other choices: none, ssh
ðŸ“Œ  Using Docker driver with root privileges
ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
ðŸšœ  Pulling base image v0.0.45 ...
ðŸ”¥  Creating docker container (CPUs=2, Memory=4096MB) ...
ðŸ³  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
ðŸ”Ž  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
```

### Prompts IN / OUT
- What can i add to make this debug better

- Output
- Enable Verbose Logging: Start Minikube with verbose logging to get more detailed output.

```
- minikube start --memory=4096 --cpus=2 --v=7
- minikube logs
- minikube start --memory=4096 --cpus=2 --kubernetes-version=v1.31.0
```

- minikube logs
```
dFeatureGates=["DynamicResourceAllocation"]
I0113 10:26:47.156277       1 controllermanager.go:797] "Started controller" controller="replicationcontroller-controller"
I0113 10:26:47.156340       1 replica_set.go:217] "Starting controller" logger="replicationcontroller-controller" name="replicationcontroller"
I0113 10:26:47.156591       1 shared_informer.go:313] Waiting for caches to sync for ReplicationController
I0113 10:26:47.164107       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0113 10:26:47.173685       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0113 10:26:47.192888       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0113 10:26:47.207085       1 shared_informer.go:320] Caches are synced for endpoint
I0113 10:26:47.207647       1 shared_informer.go:320] Caches are synced for GC
I0113 10:26:47.209606       1 shared_informer.go:320] Caches are synced for daemon sets
I0113 10:26:47.212229       1 shared_informer.go:320] Caches are synced for node
I0113 10:26:47.212439       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I0113 10:26:47.212622       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I0113 10:26:47.213303       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0113 10:26:47.213323       1 shared_informer.go:320] Caches are synced for cidrallocator
I0113 10:26:47.213529       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 10:26:47.214529       1 shared_informer.go:320] Caches are synced for HPA
I0113 10:26:47.215419       1 shared_informer.go:320] Caches are synced for PV protection
I0113 10:26:47.217313       1 shared_informer.go:320] Caches are synced for disruption
I0113 10:26:47.219891       1 shared_informer.go:320] Caches are synced for taint
I0113 10:26:47.220089       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0113 10:26:47.220291       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0113 10:26:47.220362       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0113 10:26:47.223242       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0113 10:26:47.228711       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0113 10:26:47.228768       1 shared_informer.go:320] Caches are synced for persistent volume
I0113 10:26:47.230866       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0113 10:26:47.230947       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="42.509Âµs"
I0113 10:26:47.231950       1 shared_informer.go:320] Caches are synced for PVC protection
I0113 10:26:47.252660       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0113 10:26:47.252668       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0113 10:26:47.253758       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0113 10:26:47.253907       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0113 10:26:47.257235       1 shared_informer.go:320] Caches are synced for service account
I0113 10:26:47.257388       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0113 10:26:47.257399       1 shared_informer.go:320] Caches are synced for stateful set
I0113 10:26:47.257408       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0113 10:26:47.257417       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0113 10:26:47.257426       1 shared_informer.go:320] Caches are synced for ReplicationController
I0113 10:26:47.257433       1 shared_informer.go:320] Caches are synced for TTL
I0113 10:26:47.259300       1 shared_informer.go:320] Caches are synced for crt configmap
I0113 10:26:47.260003       1 shared_informer.go:320] Caches are synced for ephemeral
I0113 10:26:47.263583       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0113 10:26:47.264785       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0113 10:26:47.265881       1 shared_informer.go:320] Caches are synced for deployment
I0113 10:26:47.267096       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0113 10:26:47.271537       1 shared_informer.go:320] Caches are synced for expand
I0113 10:26:47.310343       1 shared_informer.go:320] Caches are synced for namespace
I0113 10:26:47.334455       1 shared_informer.go:320] Caches are synced for TTL after finished
I0113 10:26:47.350811       1 shared_informer.go:320] Caches are synced for job
I0113 10:26:47.357857       1 shared_informer.go:320] Caches are synced for cronjob
I0113 10:26:47.364521       1 shared_informer.go:320] Caches are synced for resource quota
I0113 10:26:47.447055       1 shared_informer.go:320] Caches are synced for resource quota
I0113 10:26:47.455727       1 shared_informer.go:320] Caches are synced for attach detach
I0113 10:26:47.894027       1 shared_informer.go:320] Caches are synced for garbage collector
I0113 10:26:47.958108       1 shared_informer.go:320] Caches are synced for garbage collector
I0113 10:26:47.958135       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-controller-manager [bcb0941a6a65] <==
I0113 09:43:41.252731       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0113 09:43:41.269152       1 shared_informer.go:320] Caches are synced for stateful set
I0113 09:43:41.281343       1 shared_informer.go:320] Caches are synced for PV protection
I0113 09:43:41.287610       1 shared_informer.go:320] Caches are synced for ephemeral
I0113 09:43:41.290220       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0113 09:43:41.291312       1 shared_informer.go:320] Caches are synced for endpoint
I0113 09:43:41.291444       1 shared_informer.go:320] Caches are synced for persistent volume
I0113 09:43:41.291793       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0113 09:43:41.291799       1 shared_informer.go:320] Caches are synced for service account
I0113 09:43:41.291936       1 shared_informer.go:320] Caches are synced for HPA
I0113 09:43:41.292384       1 shared_informer.go:320] Caches are synced for PVC protection
I0113 09:43:41.292392       1 shared_informer.go:320] Caches are synced for daemon sets
I0113 09:43:41.293723       1 shared_informer.go:320] Caches are synced for cronjob
I0113 09:43:41.297665       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0113 09:43:41.298896       1 shared_informer.go:320] Caches are synced for deployment
I0113 09:43:41.298903       1 shared_informer.go:320] Caches are synced for attach detach
I0113 09:43:41.299388       1 shared_informer.go:320] Caches are synced for node
I0113 09:43:41.299702       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I0113 09:43:41.299794       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I0113 09:43:41.299839       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0113 09:43:41.299851       1 shared_informer.go:320] Caches are synced for cidrallocator
I0113 09:43:41.303243       1 shared_informer.go:320] Caches are synced for taint
I0113 09:43:41.303327       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0113 09:43:41.303372       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0113 09:43:41.303400       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0113 09:43:41.308303       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0113 09:43:41.308332       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 09:43:41.308643       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 09:43:41.312104       1 shared_informer.go:320] Caches are synced for namespace
I0113 09:43:41.341152       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0113 09:43:41.341231       1 shared_informer.go:320] Caches are synced for disruption
I0113 09:43:41.356625       1 shared_informer.go:320] Caches are synced for resource quota
I0113 09:43:41.399397       1 shared_informer.go:320] Caches are synced for resource quota
I0113 09:43:41.444358       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0113 09:43:41.444390       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0113 09:43:41.444444       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0113 09:43:41.444539       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0113 09:43:41.446377       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0113 09:43:41.463215       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 09:43:41.490758       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0113 09:43:41.872429       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 09:43:41.927692       1 shared_informer.go:320] Caches are synced for garbage collector
I0113 09:43:41.954227       1 shared_informer.go:320] Caches are synced for garbage collector
I0113 09:43:41.954246       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0113 09:43:42.423059       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="216.127128ms"
I0113 09:43:42.472732       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="49.44594ms"
I0113 09:43:42.489338       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="16.563406ms"
I0113 09:43:42.489448       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="32.902Âµs"
I0113 09:43:44.380974       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="79.569Âµs"
I0113 09:43:45.380416       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="7.505098ms"
I0113 09:43:45.381717       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="71.533Âµs"
I0113 09:43:47.392633       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 09:48:53.285058       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 09:54:00.922542       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 09:59:07.219198       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 10:04:12.572578       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 10:09:18.326340       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 10:14:23.870781       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 10:19:30.845374       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0113 10:24:36.769770       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [68c5d13075ec] <==
I0113 09:43:43.056149       1 server_linux.go:66] "Using iptables proxy"
I0113 09:43:43.243103       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0113 09:43:43.243229       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0113 09:43:43.373101       1 server_linux.go:146] "No iptables support for family" ipFamily="IPv6"
I0113 09:43:43.373468       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0113 09:43:43.373514       1 server_linux.go:169] "Using iptables Proxier"
I0113 09:43:43.378261       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0113 09:43:43.378858       1 server.go:483] "Version info" version="v1.31.0"
I0113 09:43:43.378993       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0113 09:43:43.394630       1 config.go:197] "Starting service config controller"
I0113 09:43:43.396397       1 shared_informer.go:313] Waiting for caches to sync for service config
I0113 09:43:43.396587       1 config.go:326] "Starting node config controller"
I0113 09:43:43.399268       1 shared_informer.go:313] Waiting for caches to sync for node config
I0113 09:43:43.397488       1 config.go:104] "Starting endpoint slice config controller"
I0113 09:43:43.399282       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0113 09:43:43.499445       1 shared_informer.go:320] Caches are synced for service config
I0113 09:43:43.499445       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0113 09:43:43.499459       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [715961229e9d] <==
I0113 10:26:41.562138       1 server_linux.go:66] "Using iptables proxy"
I0113 10:26:44.130890       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0113 10:26:44.165074       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0113 10:26:44.496301       1 server_linux.go:146] "No iptables support for family" ipFamily="IPv6"
I0113 10:26:44.496346       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0113 10:26:44.496372       1 server_linux.go:169] "Using iptables Proxier"
I0113 10:26:44.499329       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0113 10:26:44.499591       1 server.go:483] "Version info" version="v1.31.0"
I0113 10:26:44.499603       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0113 10:26:44.504614       1 config.go:197] "Starting service config controller"
I0113 10:26:44.504658       1 shared_informer.go:313] Waiting for caches to sync for service config
I0113 10:26:44.504683       1 config.go:104] "Starting endpoint slice config controller"
I0113 10:26:44.504689       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0113 10:26:44.505174       1 config.go:326] "Starting node config controller"
I0113 10:26:44.505181       1 shared_informer.go:313] Waiting for caches to sync for node config
I0113 10:26:44.605743       1 shared_informer.go:320] Caches are synced for node config
I0113 10:26:44.605776       1 shared_informer.go:320] Caches are synced for service config
I0113 10:26:44.605802       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [5925d62aec7d] <==
W0113 09:43:34.503927       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0113 09:43:34.503957       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0113 09:43:34.504199       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0113 09:43:34.543325       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0113 09:43:34.547335       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0113 09:43:34.555527       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0113 09:43:34.555772       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0113 09:43:34.558844       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0113 09:43:34.561970       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0113 09:43:34.563994       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0113 09:43:34.565262       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0113 09:43:34.565293       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0113 09:43:34.565351       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0113 09:43:34.565618       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0113 09:43:34.565688       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0113 09:43:34.565798       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0113 09:43:34.565865       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0113 09:43:34.566114       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0113 09:43:34.566690       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0113 09:43:34.566905       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0113 09:43:34.566936       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0113 09:43:34.568290       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0113 09:43:34.568431       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0113 09:43:34.568438       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0113 09:43:34.568334       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0113 09:43:34.568585       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0113 09:43:34.568612       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0113 09:43:34.568768       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0113 09:43:34.568803       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0113 09:43:34.568954       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0113 09:43:34.568980       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0113 09:43:34.569072       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0113 09:43:34.569163       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0113 09:43:34.568954       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0113 09:43:34.569288       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0113 09:43:34.569411       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0113 09:43:34.569476       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0113 09:43:34.569566       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0113 09:43:34.569582       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0113 09:43:35.418050       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0113 09:43:35.418158       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0113 09:43:35.593433       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0113 09:43:35.593710       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0113 09:43:35.607582       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0113 09:43:35.607801       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0113 09:43:35.614638       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0113 09:43:35.614871       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0113 09:43:35.629601       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0113 09:43:35.629790       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0113 09:43:35.668338       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0113 09:43:35.668623       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0113 09:43:35.693953       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0113 09:43:35.694274       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0113 09:43:35.717170       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0113 09:43:35.717219       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
I0113 09:43:37.359479       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0113 10:26:23.359908       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0113 10:26:23.359969       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0113 10:26:23.360262       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0113 10:26:23.361023       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [c8549e9bfb46] <==
I0113 10:26:42.110744       1 serving.go:386] Generated self-signed cert in-memory
W0113 10:26:43.909942       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0113 10:26:43.909997       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0113 10:26:43.910009       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0113 10:26:43.910018       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0113 10:26:44.110796       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0113 10:26:44.136121       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0113 10:26:44.148686       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0113 10:26:44.148790       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0113 10:26:44.148813       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0113 10:26:44.148829       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0113 10:26:44.249693       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jan 13 10:26:34 minikube kubelet[2178]: E0113 10:26:34.503821    2178 generic.go:238] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = Unknown desc = error during connect: Get \"http://%2Fvar%2Frun%2Fdocker.sock/v1.43/containers/json?all=1&filters=%7B%22label%22%3A%7B%22io.kubernetes.docker.type%3Dpodsandbox%22%3Atrue%7D%7D\": read unix @->/var/run/docker.sock: read: connection reset by peer"
Jan 13 10:26:34 minikube kubelet[2178]: I0113 10:26:34.510252    2178 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused - error from a previous attempt: unexpected EOF"
Jan 13 10:26:34 minikube kubelet[2178]: I0113 10:26:34.510984    2178 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:34 minikube kubelet[2178]: E0113 10:26:34.544678    2178 kubelet_node_status.go:535] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused - error from a previous attempt: read tcp 192.168.49.2:35812->192.168.49.2:8443: read: connection reset by peer"
Jan 13 10:26:34 minikube kubelet[2178]: E0113 10:26:34.545066    2178 kubelet_node_status.go:535] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:34 minikube kubelet[2178]: E0113 10:26:34.545301    2178 kubelet_node_status.go:535] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:34 minikube kubelet[2178]: E0113 10:26:34.545455    2178 kubelet_node_status.go:535] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:34 minikube kubelet[2178]: E0113 10:26:34.545477    2178 kubelet_node_status.go:522] "Unable to update node status" err="update node status exceeds retry count"
Jan 13 10:26:34 minikube kubelet[2178]: E0113 10:26:34.946794    2178 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="1.6s"
Jan 13 10:26:35 minikube kubelet[2178]: W0113 10:26:35.718304    2178 logging.go:55] [core] [Channel #1 SubChannel #2]grpc: addrConn.createTransport failed to connect to {Addr: "/var/run/cri-dockerd.sock", ServerName: "localhost", }. Err: connection error: desc = "error reading server preface: read unix @->/run/cri-dockerd.sock: read: connection reset by peer"
Jan 13 10:26:35 minikube kubelet[2178]: E0113 10:26:35.718767    2178 log.go:32] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: read unix @->/run/cri-dockerd.sock: read: connection reset by peer\"" filter="nil"
Jan 13 10:26:35 minikube kubelet[2178]: E0113 10:26:35.718863    2178 kuberuntime_sandbox.go:305] "Failed to list pod sandboxes" err="rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: read unix @->/run/cri-dockerd.sock: read: connection reset by peer\""
Jan 13 10:26:35 minikube kubelet[2178]: E0113 10:26:35.718935    2178 generic.go:238] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: read unix @->/run/cri-dockerd.sock: read: connection reset by peer\""
Jan 13 10:26:36 minikube kubelet[2178]: E0113 10:26:36.547587    2178 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="3.2s"
Jan 13 10:26:37 minikube kubelet[2178]: I0113 10:26:37.175432    2178 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:37 minikube kubelet[2178]: I0113 10:26:37.430009    2178 scope.go:117] "RemoveContainer" containerID="fdf5df8b1c18c225009eefa4634ae26ac4b2f574b97bf8123360ee06dd972a1f"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.479732    2178 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.479978    2178 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.480537    2178 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.480910    2178 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.481071    2178 status_manager.go:851] "Failed to get status for pod" podUID="7b89cd3b-01cf-4ffe-952c-e600f0cca4c8" pod="kube-system/coredns-6f6b679f8f-2hs27" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-2hs27\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.481232    2178 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.481385    2178 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.481734    2178 status_manager.go:851] "Failed to get status for pod" podUID="2ebe54a3-881c-4056-8bae-de97ca74c8e0" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.481904    2178 status_manager.go:851] "Failed to get status for pod" podUID="7b89cd3b-01cf-4ffe-952c-e600f0cca4c8" pod="kube-system/coredns-6f6b679f8f-2hs27" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-2hs27\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.483474    2178 status_manager.go:851] "Failed to get status for pod" podUID="2ebe54a3-881c-4056-8bae-de97ca74c8e0" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.484417    2178 status_manager.go:851] "Failed to get status for pod" podUID="7b89cd3b-01cf-4ffe-952c-e600f0cca4c8" pod="kube-system/coredns-6f6b679f8f-2hs27" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-2hs27\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.484801    2178 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.485074    2178 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.486089    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3499b4deae23214674a102f574eadb5288840a629220f9d82c43fe1e7a828773"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.486115    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="438e0c40c64c5a39ae93110773fea228710c37d925c15b1d33eeca7ab6305f91"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.486133    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2763e5ef50ded6649a7db2f4c0df03039a8b666a1393c0b02eebaf0354fb3269"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.486151    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="948adb181a1cd82346f0cc7a703f676fb80b61ce54c4cc8b8c8d08b991186b91"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.486167    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a8d6da38272385caf41e59b5291ecd3266adf7a90acea0b1a1476d9445176a97"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.486183    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1918fa04e34fcd12f5839c6a0b45349a3b277469df14de5c943445817957a568"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.486198    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="62db94e69176686d4b641a5b965901702173c911e31caacd36dde3735bed5bc9"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.487113    2178 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.487292    2178 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.487537    2178 status_manager.go:851] "Failed to get status for pod" podUID="2ebe54a3-881c-4056-8bae-de97ca74c8e0" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.487742    2178 status_manager.go:851] "Failed to get status for pod" podUID="48e8a492-c851-4002-ba23-26e879e1212f" pod="kube-system/kube-proxy-c6kkz" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-c6kkz\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.487914    2178 status_manager.go:851] "Failed to get status for pod" podUID="7b89cd3b-01cf-4ffe-952c-e600f0cca4c8" pod="kube-system/coredns-6f6b679f8f-2hs27" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-2hs27\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.488113    2178 status_manager.go:851] "Failed to get status for pod" podUID="40f5f661ab65f2e4bfe41ac2993c01de" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.488275    2178 status_manager.go:851] "Failed to get status for pod" podUID="2ebe54a3-881c-4056-8bae-de97ca74c8e0" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.488445    2178 status_manager.go:851] "Failed to get status for pod" podUID="48e8a492-c851-4002-ba23-26e879e1212f" pod="kube-system/kube-proxy-c6kkz" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-c6kkz\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.488608    2178 status_manager.go:851] "Failed to get status for pod" podUID="7b89cd3b-01cf-4ffe-952c-e600f0cca4c8" pod="kube-system/coredns-6f6b679f8f-2hs27" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-2hs27\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.488772    2178 status_manager.go:851] "Failed to get status for pod" podUID="e039200acb850c82bb901653cc38ff6e" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.488935    2178 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: I0113 10:26:39.489094    2178 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jan 13 10:26:39 minikube kubelet[2178]: E0113 10:26:39.748471    2178 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="6.4s"
Jan 13 10:26:40 minikube kubelet[2178]: I0113 10:26:40.468571    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0ff9bce2ae7cd533e0d8d00c6a08315f7ef692bddabf71f78147f2d6c05723ea"
Jan 13 10:26:40 minikube kubelet[2178]: E0113 10:26:40.745232    2178 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events\": dial tcp 192.168.49.2:8443: connect: connection refused" event="&Event{ObjectMeta:{etcd-minikube.181a39a4381ec1f9  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:kube-system,Name:etcd-minikube,UID:a5363f4f31e043bdae3c93aca4991903,APIVersion:v1,ResourceVersion:,FieldPath:spec.containers{etcd},},Reason:Unhealthy,Message:Readiness probe failed: Get \"http://127.0.0.1:2381/readyz\": dial tcp 127.0.0.1:2381: connect: connection refused,Source:EventSource{Component:kubelet,Host:minikube,},FirstTimestamp:2025-01-13 10:26:23.783707129 +0000 UTC m=+2566.733327791,LastTimestamp:2025-01-13 10:26:23.783707129 +0000 UTC m=+2566.733327791,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:minikube,}"
Jan 13 10:26:40 minikube kubelet[2178]: I0113 10:26:40.822321    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f76c1bf1720f4d3007f7229765c147bd19739f26c980a77eb48ae87b9498cdd0"
Jan 13 10:26:40 minikube kubelet[2178]: I0113 10:26:40.901121    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7859ef19f33e4c35b94b78d80a90dd978850354c430df66a0a517f065acf0f6b"
Jan 13 10:26:41 minikube kubelet[2178]: I0113 10:26:41.092591    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3f6ce9f5021065862f1bc2d990107f32d6bec6be7d57119fafe121e24844a515"
Jan 13 10:26:41 minikube kubelet[2178]: I0113 10:26:41.108113    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1aedd946c4284bd0d388d102f60e68e86a9dd49a76d8279e1197c0acdf70944b"
Jan 13 10:26:41 minikube kubelet[2178]: I0113 10:26:41.318950    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1fbbba311210e7348e2e10f2ced9f21df844adb86472943b6b568635a03d8baf"
Jan 13 10:26:41 minikube kubelet[2178]: I0113 10:26:41.328559    2178 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8fb1f70f030a2ab5489ee80c31aef78b6833ce8ac05067c14acda9e5b0734c07"
Jan 13 10:26:43 minikube kubelet[2178]: E0113 10:26:43.899737    2178 reflector.go:158] "Unhandled Error" err="object-\"kube-system\"/\"kube-root-ca.crt\": Failed to watch *v1.ConfigMap: unknown (get configmaps)" logger="UnhandledError"
Jan 13 10:26:43 minikube kubelet[2178]: E0113 10:26:43.899803    2178 reflector.go:158] "Unhandled Error" err="object-\"kube-system\"/\"coredns\": Failed to watch *v1.ConfigMap: unknown (get configmaps)" logger="UnhandledError"
Jan 13 10:26:43 minikube kubelet[2178]: E0113 10:26:43.984366    2178 reflector.go:158] "Unhandled Error" err="object-\"kube-system\"/\"kube-proxy\": Failed to watch *v1.ConfigMap: unknown (get configmaps)" logger="UnhandledError"


==> storage-provisioner [7e757d4adc60] <==
I0113 09:44:12.627821       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0113 09:44:12.640980       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0113 09:44:12.641844       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0113 09:44:12.652975       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0113 09:44:12.653114       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_911e1a6e-83d4-481b-9f67-2355c9890f0c!
I0113 09:44:12.655125       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"4a7508d5-362e-43dc-afe9-832b2642d315", APIVersion:"v1", ResourceVersion:"425", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_911e1a6e-83d4-481b-9f67-2355c9890f0c became leader
I0113 09:44:12.754602       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_911e1a6e-83d4-481b-9f67-2355c9890f0c!


==> storage-provisioner [8b3a8154bae3] <==
I0113 10:26:41.563941       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0113 10:26:44.128926       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0113 10:26:44.128981       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0113 10:27:01.520499       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0113 10:27:01.521201       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"4a7508d5-362e-43dc-afe9-832b2642d315", APIVersion:"v1", ResourceVersion:"2553", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_963302c1-b259-40cf-ab2a-cb21034390ec became leader
I0113 10:27:01.521389       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_963302c1-b259-40cf-ab2a-cb21034390ec!
I0113 10:27:01.622161       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_963302c1-b259-40cf-ab2a-cb21034390ec!

```
- mitigation
```sh
# chmod +x /workspaces/grafana-dashboard/6_Symbols/1_Init/minikube-startup.sh
# /workspaces/grafana-dashboard/6_Symbols/1_Init/minikube-startup.sh
```
---

## Section 2: ðŸ› ï¸ Commands for Environment Stage: Create the Resource
```sh
- kubectl create namespace grafana-monitoring 
- helm repo add bitnami https://charts.bitnami.com/bitnami  
- helm repo update 
- cd 6_Symbols/2_Grafana 
- clear
- kubectl apply -f grafana-datasource.yaml -n grafana-monitoring 
- kubectl apply -f configmap_randomwalk.yaml -n grafana-monitoring
- kubectl create secret generic datasource-secret --from-file=datasource-secret.yaml -n grafana-monitoring
- kubectl apply -f grafana-dashboard-provisioning.yaml -n grafana-monitoring 
- helm install grafana bitnami/grafana --namespace grafana-monitoring -f grafana-dashboard-deployment.yaml 
```

### âš™ï¸ Configuration for Environment 
- **minikube**: running in docker
- **codespaces**: running in the host Azure setup with helm installed with the repos
- **configmaps**: datasources,secrets,dashboards
- **bitnami**: running in the host Azure setup
-configmaprandomwalk.yaml
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-config
  labels:
    grafana_dashboard: "1"
data:
  grafana_dashboard_config: |
    {
       "dashboard": {
         "id": null,
         "title": "Random Walk Dashboard",
         "tags": [],
         "timezone": "browser",
         "schemaVersion": 16,
         "version": 0,
         "panels": [
           {
             "type": "graph",
             "title": "Random Walk",
             "gridPos": { "x": 0, "y": 0, "w": 24, "h": 9 },
             "datasource": "Random Walk Data",
             "targets": [
               {
                 "refId": "A",
                 "scenarioId": "random_walk"
               }
             ]
           }
         ]
       }
     }
```
- grafana-datasource.yaml
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  labels:
    grafana_datasource: "1"
data:
  random-walk-datasource.yaml: |
    apiVersion: 1
    datasources:
      - name: Random Walk Data
        type: testdata
        access: proxy
        isDefault: true
```
- datasource-secret.yaml
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: grafana-datasource-secret
type: Opaque
data:
  datasource.yaml: |
    apiVersion: 1
    datasources:
      - name: Random Walk
        type: grafana-random-walk
        access: proxy
        url: http://random-walk:8080
        isDefault: true
        jsonData:
          timeInterval: 5s
        secureJsonData:
          basicAuthPassword: cGFzc3dvcmQ=
            tlsSkipVerify: true
          basicAuthUser: dXNlcg==
            tlsSkipVerify: true
```
- grafana-dashboard-provisioning.yaml
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards-provisioning
data:
  dashboards.yaml: |
      apiVersion: 1
      providers:
        - name: "grafanadashboardconfig"
          orgId: 1
          folder: ""
          type: "file"
          options:
              path: /opt/bitnami/grafana/dashboards/grafana_dashboard_config.yaml
```
- grafana-dashboard-deployment.yaml 
```yaml
admin:
  ## @param admin.user Grafana admin username
  ##
  user: "admin"
  ## @param admin.password Admin password. If a password is not provided a random password will be generated
  ##
  ## Test this to make sure the values yaml used your values not the default values...if the structure fails it defaults back to default values
  password: "your-password"

## @section Grafana Deployment parameters
grafana:
  envVars:
    - name: GF_LOG_LEVEL
      value: debug
  ## @param grafana.replicaCount Number of Grafana nodes
  ##
  replicaCount: 1
  persistence:
    enabled: true
    size: 100Mi
  service:
    type: NodePort
    port: 3000
    nodePort: 30000
  extraVolumes:
    - name: grafana-storage
      emptyDir: {}
  volumeMounts:
    - name: grafana-storage
      mountPath: /opt/bitnami/grafana
    - name: grafana-dashboard-config
      mountPath: /opt/bitnami/grafana/dashboards
      readOnly: true
    - name: grafana-datasources
      mountPath: /opt/bitnami/grafana/provisioning/datasources
  volumes:
    - name: grafana-dashboard-config
      configMap:
        name: grafana-dashboard-config
        items:
          - key: grafana_dashboard_config.json
            path: grafana_dashboard_config.json
    - name: grafana-datasources
      configMap:
        name: grafana-datasources

          
dashboardsProvider:
  enabled: true
  configMapName: grafana-dashboard-config

dashboardsConfigMaps:
   - configMapName: grafana-dashboard-config
     fileName: grafana_dashboard_config.json
   - configMapName: grafana-dashboards-provisioning
     fileName: dashboards.yaml
  

## Service parameters
##
service:
  ## @param service.type Kubernetes Service type
  ##
  type: ClusterIP
  ## @param service.clusterIP Grafana service Cluster IP
  ## e.g.:
  ## clusterIP: None
  ##
  # clusterIP: ""
  ## @param service.ports.grafana Grafana service port
  ##
  ports:
    grafana: 3000
```
### ðŸƒâ€â™€ï¸ Run Output for Environment 

```
@rifaterdemsahin âžœ /workspaces/grafana-dashboard/6_Sym
bols/2_Grafana (main) $ helm upgrade grafana bit
nami/grafana --namespace grafana-monitoring -f g
rafana-dashboard-deployment.yaml 
Release "grafana" has been upgraded. Happy Helming!
NAME: grafana
LAST DEPLOYED: Mon Jan 13 11:59:00 2025
NAMESPACE: grafana-monitoring
STATUS: deployed
REVISION: 2
TEST SUITE: None
NOTES:
CHART NAME: grafana
CHART VERSION: 11.4.2
APP VERSION: 11.4.0

Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami for more information.

** Please be patient while the chart is being deployed **

1. Get the application URL by running these commands:
    echo "Browse to http://127.0.0.1:8080"
    kubectl port-forward svc/grafana 8080:3000 &

2. Get the admin credentials:

    echo "User: admin"
    echo "Password: $(kubectl get secret grafana-admin --namespace grafana-monitoring -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 -d)"
# Note: Do not include grafana.validateValues.database here. See https://github.com/bitnami/charts/issues/20629


WARNING: There are "resources" sections in the chart not set. Using "resourcesPreset" is not recommended for production. For production installations, please set the following values according to your workload needs:
  - grafana.resources
+info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
@rifaterdemsahin âžœ /workspaces/grafana-dashboard/6_Symbols/2_Grafana (main) $ 
```
### prompts:
- what is missing in these files to create the dashboards ^

promptOutput:

It looks like you have most of the necessary configurations in place, but there are a few potential areas to check and adjust:

1. **ConfigMap for Dashboards**:
   - Ensure the `grafana_dashboard_config.json` key in the `grafana-dashboard-config` ConfigMap is correctly referenced and formatted. The key should match the file name used in the volume mount.

2. **Datasource Configuration**:
   - Verify that the datasource configuration in `grafana-datasource.yaml` and `datasource-secret.yaml` is correctly set up. The datasource name and type should match what is referenced in the dashboard configuration.

3. **Secret Configuration**:
   - Ensure the secret data in `datasource-secret.yaml` is correctly encoded in base64. The `basicAuthUser` and `basicAuthPassword` fields should be properly encoded.

4. **Helm Chart Values**:
   - Check the values in `grafana-dashboard-deployment.yaml` to ensure they are correctly set. Specifically, verify the `extraVolumes`, `volumeMounts`, and `volumes` sections to ensure they correctly reference the ConfigMaps and secrets.

5. **Provisioning Configuration**:
   - Ensure the `dashboardsProvider` and `dashboardsConfigMaps` sections are correctly configured to provision the dashboards from the ConfigMap.

6. **Service Configuration**:
   - Verify the service configuration to ensure Grafana is accessible. The `NodePort` and `ClusterIP` settings should be correctly set based on your environment.

Here is a checklist to help you verify these configurations:

- [ ] `grafana_dashboard_config.json` key in `grafana-dashboard-config` ConfigMap is correctly referenced.
- [ ] Datasource name and type in `grafana-datasource.yaml` and `datasource-secret.yaml` match the dashboard configuration.
- [ ] Secret data in `datasource-secret.yaml` is correctly encoded in base64.
- [ ] Values in `grafana-dashboard-deployment.yaml` are correctly set, especially `extraVolumes`, `volumeMounts`, and `volumes`.
- [ ] `dashboardsProvider` and `dashboardsConfigMaps` sections are correctly configured.
- [ ] Service configuration is correctly set for accessibility.

- Double-checking these areas should help ensure your Grafana dashboards are created successfully. If you encounter any specific errors or issues, feel free to share them, and I can help troubleshoot further!
---

## Section 3 : ðŸ› ï¸ Commands for Environment Stage : Debug
```sh
- kubectl get pods -n grafana-monitoring 
- kubectl get secret grafana-admin --namespace grafana-monitoring -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 -d
- kubectl port-forward -n grafana-monitoring svc/grafana 3000:3000  
- kubectl get pod -n grafana-monitoring | grep "Running" | awk '{print $1}'
- kubectl exec -it grafana-7c4977f889-5zzsc -n grafana-monitoring -- /bin/bash
- kubectl logs -n grafana-monitoring grafana-76b66bfbcd-bws55

```
### âš™ï¸ Configuration for Environment 
- **X**: Y
### ðŸƒâ€â™€ï¸ Run Output for Environment 
- **Date**: `2025-01-11`
- **Status**: âœ… Successful
- **Logs**:
```
Server started on PORT 3000
Connected to database successfully
```
###  prompts:
- how do we fix the issue above ^
- promptOutput:

It looks like your Grafana pod is stuck in a "Pending" state. This usually happens when there are insufficient resources available to schedule the pod. Here are a few steps you can take to troubleshoot this issue:

1. **Check Node Resources**: Ensure that your nodes have enough CPU and memory resources available.
   ```sh
   kubectl describe node <node-name>
   ```

2. **Check Pod Events**: Look at the events for the pod to get more details on why it's pending.
   ```sh
   kubectl describe pod grafana-76b66bfbcd-bws55 -n grafana-monitoring
   ```

3. **Check Resource Requests and Limits**: Verify that the resource requests and limits for the Grafana pod are set correctly in your deployment configuration.
   ```yaml
   resources:
     requests:
       memory: "256Mi"
       cpu: "500m"
     limits:
       memory: "512Mi"
       cpu: "1"
   ```

4. **Check for Node Selectors or Taints**: Ensure that there are no node selectors or taints that are preventing the pod from being scheduled on any node.
   ```sh
   kubectl describe pod grafana-76b66bfbcd-bws55 -n grafana-monitoring
   ```

5. **Check for Pending PVCs**: If your pod uses Persistent Volume Claims (PVCs), make sure they are bound and available.
   ```sh
   kubectl get pvc -n grafana-monitoring
   ```

@rifaterdemsahin âžœ /workspaces/grafana-dashboard (main) $ kubectl get pvc -n grafana-monitoring
NAME      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
grafana   Pending                                      standard       <unset>                 82m
@rifaterdemsahin âžœ /

It looks like your Persistent Volume Claim (PVC) for Grafana is still in a "Pending" state. This could be the reason why your pod is also pending. Here are a few steps to troubleshoot this issue:

1. **Check Storage Class**: Ensure that the storage class `standard` is available and properly configured in your cluster.
   ```sh
   kubectl get storageclass
   ```

   @rifaterdemsahin âžœ /workspaces/grafana-dashboard (main) $ kubectl get storageclass
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  174m

2. **Describe the PVC**: Get more details about why the PVC is pending.
   ```sh
   kubectl describe pvc grafana -n grafana-monitoring
   ```

3. **Check for Available PVs**: Make sure there are Persistent Volumes (PVs) available that match the requirements of your PVC.
   ```sh
   kubectl get pv
   ```

4. **Check PV Events**: Look at the events for the PV to see if there are any issues.
   ```sh
   kubectl describe pv <pv-name>
   ```

5. **Verify Storage Provisioner**: Ensure that the storage provisioner for the `standard` storage class is running and healthy.

The output indicates that the PVC is waiting for a volume to be created by the external provisioner `k8s.io/minikube-hostpath`. Here are a few steps to resolve this issue:

1. **Verify Provisioner Pod**: Ensure that the Minikube hostpath provisioner pod is running. Since the `kubectl get pods -n kube-system | grep hostpath` command returned no results, it seems the provisioner might not be running.
   ```sh
   kubectl get pods -n kube-system
   ```

2. **Check Provisioner Logs**: If the provisioner pod is found, check its logs for any errors.
   ```sh
   kubectl logs <provisioner-pod-name> -n kube-system
   ```

3. **Restart Minikube**: Sometimes, restarting Minikube can help resolve issues with the provisioner.
   ```sh
   minikube stop
   minikube start
   ```

4. **Manual PV Creation**: If the provisioner is not working, you can manually create a Persistent Volume (PV) that matches the PVC requirements.
   ```yaml
   apiVersion: v1
   kind: PersistentVolume
   metadata:
     name: grafana-pv
   spec:
     capacity:
       storage: 1Gi
     accessModes:
       - ReadWriteOnce
     hostPath:
       path: "/mnt/data"
   ```

5. **Reapply PVC**: After creating the PV, reapply the PVC to bind it to the newly created PV.
   ```sh
   kubectl delete pvc grafana -n grafana-monitoring
   kubectl apply -f <pvc-definition-file>
   ```

Try these steps and see if they help resolve the issue. If you need further assistance, feel free to ask!